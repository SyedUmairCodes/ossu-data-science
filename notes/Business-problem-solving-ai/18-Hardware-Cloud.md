Machine learning (ML) models can be trained faster by using parallelization, which involves dividing tasks among multiple processors. Parallelization can reduce training time from weeks to hours, allowing for more experimentation with different models and more frequent retraining on new data. 

It also enables more fine-tuning of models for better performance. GPUs are better suited for deep learning tasks than CPUs because they are optimized for processing large amounts of memory, which is essential for handling large datasets. 

GPUs also have larger memory bandwidth than CPUs, which makes them faster at processing large batches of data. When setting up a system for deep learning, it is important to choose GPUs that are specifically supported by the algorithms you are using.

## GPU Platforms
GPU manufacturers like Nvidia and AMD offer GPUs for deep learning tasks. Not all GPUs are compatible with all deep learning algorithms. Currently, Nvidia GPUs and CUDA (Compute Unified Device Architecture) are the dominant combination for deep learning.

> CUDA works with all Nvidia GPUs from the G8x series onwards and is compatible with most operating systems. 

While AMD's ROCm platform aims to provide a unified programming environment for both AMD and Nvidia GPUs, it has not yet gained widespread adoption.

## Cloud Platforms
Cloud platforms like Amazon Web Services (AWS) and Microsoft Azure allow you to provision virtual machine instances of GPUs, which you can easily scale up or down as needed.

This can be a more cost-effective option than purchasing your own GPU hardware, especially for experimentation and prototyping. However, the cost of cloud services can add up quickly, so it is important to consider your needs carefully.